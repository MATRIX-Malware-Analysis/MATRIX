{
    "type": "attack-pattern",
    "spec_version": "2.1",
    "id": "attack-pattern--8502eb3c-4a6a-4fb0-9b19-fd03ba1327ae",
    "created": "2024-08-14T16:31:43.978619Z",
    "modified": "2024-08-14T16:31:43.978623Z",
    "name": "Gather PDF Authors",
    "description": " This module downloads PDF documents and extracts the author's name from the document metadata. ",
    "external_references": [
        {
            "source_name": "metasploit",
            "url": "https://github.com/rapid7/metasploit-framework/blob/master/auxiliary/gather/http_pdf_authors.rb",
            "external_id": "http_pdf_authors.rb"
        },
        {
            "source_name": "Alternatively",
            "external_id": "multipleURLscanbeprovidedbysupplyingthe"
        },
        {
            "source_name": "ByspecifyingpdffortheURL_TYPE",
            "external_id": "themodulewilltreat"
        },
        {
            "source_name": "ByspecifyinghtmlfortheURL_TYPE",
            "external_id": "themodulewilltreat"
        },
        {
            "source_name": "fail_withFailure::BadConfig",
            "external_id": "NoURL(s)specified"
        },
        {
            "source_name": "fail_withFailure::BadConfig",
            "external_id": "\"File#{datastoreURL_LIST}doesnotexist\""
        },
        {
            "source_name": "File.open(datastoreURL_LIST",
            "external_id": "rb){|f|f.read}.split(/\\r?\\n/)"
        }
    ],
    "x_code_snippet": "##\n# This module requires Metasploit: https://metasploit.com/download\n# Current source: https://github.com/rapid7/metasploit-framework\n##\n\nclass MetasploitModule < Msf::Auxiliary\n  include Msf::Exploit::Remote::HttpClient\n  include Msf::Auxiliary::Report\n\n  def initialize(info = {})\n    super(update_info(info,\n      'Name'        => 'Gather PDF Authors',\n      'Description' => %q{\n        This module downloads PDF documents and extracts the author's\n        name from the document metadata.\n\n        This module expects a URL to be provided using the URL option.\n        Alternatively, multiple URLs can be provided by supplying the\n        path to a file containing a list of URLs in the URL_LIST option.\n\n        The URL_TYPE option is used to specify the type of URLs supplied.\n\n        By specifying 'pdf' for the URL_TYPE, the module will treat\n        the specified URL(s) as PDF documents. The module will\n        download the documents and extract the authors' names from the\n        document metadata.\n\n        By specifying 'html' for the URL_TYPE, the module will treat\n        the specified URL(s) as HTML pages. The module will scrape the\n        pages for links to PDF documents, download the PDF documents,\n        and extract the author's name from the document metadata.\n      },\n      'License'     => MSF_LICENSE,\n      'Author'      => 'bcoles'))\n\n    deregister_http_client_options\n\n    register_options(\n      [\n        OptString.new('URL', [ false, 'The target URL', '' ]),\n        OptString.new('URL_LIST', [ false, 'File containing a list of target URLs', '' ]),\n        OptEnum.new('URL_TYPE', [ true, 'The type of URL(s) specified', 'html', [ 'pdf', 'html' ] ]),\n        OptBool.new('STORE_LOOT', [ false, 'Store authors in loot', true ])\n      ])\n  end\n\n  def progress(current, total)\n    done = (current.to_f / total.to_f) * 100\n    percent = \"%3.2f%%\" % done.to_f\n    print_status \"%7s done (%d/%d files)\" % [percent, current, total]\n  end\n\n  def load_urls\n    return [ datastore['URL'] ] unless datastore['URL'].to_s.eql? ''\n\n    if datastore['URL_LIST'].to_s.eql? ''\n      fail_with Failure::BadConfig, 'No URL(s) specified'\n    end\n\n    unless File.file? datastore['URL_LIST'].to_s\n      fail_with Failure::BadConfig, \"File '#{datastore['URL_LIST']}' does not exist\"\n    end\n\n    File.open(datastore['URL_LIST'], 'rb') { |f| f.read }.split(/\\r?\\n/)\n  end\n\n  def read(data)\n    require 'pdf-reader'\n\n    Timeout.timeout(10) do\n      reader = PDF::Reader.new data\n      return parse reader\n    end\n  rescue PDF::Reader::MalformedPDFError\n    print_error \"Could not parse PDF: PDF is malformed (MalformedPDFError)\"\n    return\n  rescue PDF::Reader::UnsupportedFeatureError\n    print_error \"Could not parse PDF: PDF contains unsupported features (UnsupportedFeatureError)\"\n    return\n  rescue SystemStackError\n    print_error \"Could not parse PDF: PDF is malformed (SystemStackError)\"\n    return\n  rescue SyntaxError\n    print_error \"Could not parse PDF: PDF is malformed (SyntaxError)\"\n    return\n  rescue Timeout::Error\n    print_error \"Could not parse PDF: PDF is malformed (Timeout)\"\n    return\n  rescue => e\n    print_error \"Could not parse PDF: Unhandled exception: #{e}\"\n    return\n  end\n\n  def parse(reader)\n    # PDF\n    # print_status \"PDF Version: #{reader.pdf_version}\"\n    # print_status \"PDF Title: #{reader.info['title']}\"\n    # print_status \"PDF Info: #{reader.info}\"\n    # print_status \"PDF Metadata: #{reader.metadata}\"\n    # print_status \"PDF Pages: #{reader.page_count}\"\n\n    # Software\n    # print_status \"PDF Creator: #{reader.info[:Creator]}\"\n    # print_status \"PDF Producer: #{reader.info[:Producer]}\"\n\n    # Author\n    reader.info[:Author].class == String ? reader.info[:Author].split(/\\r?\\n/).first : ''\n  end\n\n  def run\n    urls = load_urls\n\n    if datastore['URL_TYPE'].eql? 'html'\n      urls = extract_pdf_links urls\n\n      if urls.empty?\n        print_error 'Found no links to PDF files'\n        return\n      end\n\n      print_line\n      print_good \"Found links to #{urls.size} PDF files:\"\n      print_line urls.join \"\\n\"\n      print_line\n    end\n\n    authors = extract_authors urls\n\n    print_line\n\n    if authors.empty?\n      print_status 'Found no authors'\n      return\n    end\n\n    print_good \"Found #{authors.size} authors: #{authors.join ', '}\"\n\n    return unless datastore['STORE_LOOT']\n\n    p = store_loot 'pdf.authors', 'text/plain', nil, authors.join(\"\\n\"), 'pdf.authors.txt', 'PDF authors'\n    print_good \"File saved in: #{p}\"\n  end\n\n  def extract_pdf_links(urls)\n    print_status \"Processing #{urls.size} URLs...\"\n\n    pdf_urls = []\n    urls.each_with_index do |url, index|\n      next if url.blank?\n      html = download url\n      next if html.blank?\n      doc = Nokogiri::HTML html\n      doc.search('a[href]').select { |n| n['href'][/(\\.pdf$|\\.pdf\\?)/] }.map do |n|\n        pdf_urls << URI.join(url, n['href']).to_s\n      end\n      progress(index + 1, urls.size)\n    end\n\n    pdf_urls.uniq\n  end\n\n  def extract_authors(urls)\n    print_status \"Processing #{urls.size} URLs...\"\n\n    authors = []\n    max_len = 256\n    urls.each_with_index do |url, index|\n      next if url.blank?\n      file = download url\n      next if file.blank?\n      pdf = StringIO.new\n      pdf.puts file\n      author = read pdf\n      unless author.blank?\n        print_good \"PDF Author: #{author}\"\n        if author.length > max_len\n          print_warning \"Warning: Truncated author's name at #{max_len} characters\"\n          authors << author[0...max_len]\n        else\n          authors << author\n        end\n      end\n      progress(index + 1, urls.size)\n    end\n\n    authors.uniq\n  end\nend\n",
    "x_mitre_contributors": [
        "bcoles))"
    ]
}