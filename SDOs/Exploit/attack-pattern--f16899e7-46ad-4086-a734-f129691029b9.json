{
    "type": "attack-pattern",
    "spec_version": "2.1",
    "id": "attack-pattern--f16899e7-46ad-4086-a734-f129691029b9",
    "created": "2024-08-14T16:30:44.333717Z",
    "modified": "2024-08-14T16:30:44.333721Z",
    "name": "Archive.org Stored Domain URLs",
    "description": "No description available.",
    "external_references": [
        {
            "source_name": "metasploit",
            "url": "https://github.com/rapid7/metasploit-framework/blob/master/auxiliary/scanner/http/enum_wayback.rb",
            "external_id": "enum_wayback.rb"
        }
    ],
    "x_code_snippet": "##\n# This module requires Metasploit: https://metasploit.com/download\n# Current source: https://github.com/rapid7/metasploit-framework\n##\n\nrequire 'net/http'\n\nclass MetasploitModule < Msf::Auxiliary\n  include Msf::Auxiliary::Report\n  def initialize(info = {})\n    super(update_info(info,\n      'Name' => 'Archive.org Stored Domain URLs',\n      'Description' => %q{\n          This module pulls and parses the URLs stored by Archive.org for the purpose of\n        replaying during a web assessment. Finding unlinked and old pages.\n      },\n      'Author' => [ 'mubix' ],\n      'License' => MSF_LICENSE\n    ))\n    register_options(\n      [\n        OptString.new('DOMAIN', [ true, \"Domain to request URLS for\"]),\n        OptString.new('OUTFILE', [ false, \"Where to output the list for use\"])\n      ])\n\n    register_advanced_options(\n      [\n        OptString.new('PROXY', [ false, \"Proxy server to route connection. <host>:<port>\",nil]),\n        OptString.new('PROXY_USER', [ false, \"Proxy Server User\",nil]),\n        OptString.new('PROXY_PASS', [ false, \"Proxy Server Password\",nil])\n      ])\n\n  end\n\n  def pull_urls(targetdom)\n    response = \"\"\n    pages = []\n    header = { 'User-Agent' => Rex::UserAgent.session_agent }\n    # https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server\n    clnt = Net::HTTP::Proxy(@proxysrv,@proxyport,@proxyuser,@proxypass).new(\"web.archive.org\")\n    resp = clnt.get2(\"/cdx/search/cdx?url=\"+Rex::Text.uri_encode(\"#{targetdom}/*\")+\"&fl=original\",header)\n    response << resp.body\n    response.each_line do |line|\n      pages << line.strip\n    end\n\n    pages.delete_if{|x| x==nil}\n    pages.uniq!\n    pages.sort!\n\n    for i in (0..(pages.count-1))\n      fix = pages[i].to_s.sub(':80', '')\n      pages[i] = fix\n    end\n    return pages\n  end\n\n  def write_output(data)\n    print_status(\"Writing URLs list to #{datastore['OUTFILE']}...\")\n    file_name = datastore['OUTFILE']\n    if FileTest::exist?(file_name)\n      print_status(\"OUTFILE already existed, appending..\")\n    else\n      print_status(\"OUTFILE did not exist, creating..\")\n    end\n\n    File.open(file_name, 'ab') do |fd|\n      fd.write(data)\n    end\n\n\n  end\n\n  def run\n    if datastore['PROXY']\n      @proxysrv,@proxyport = datastore['PROXY'].split(\":\")\n      @proxyuser = datastore['PROXY_USER']\n      @proxypass = datastore['PROXY_PASS']\n    else\n      @proxysrv,@proxyport = nil, nil\n    end\n\n    target = datastore['DOMAIN']\n\n    urls = []\n    print_status(\"Pulling urls from Archive.org\")\n    urls = pull_urls(target)\n\n    print_status(\"Located #{urls.count} addresses for #{target}\")\n\n    if datastore['OUTFILE']\n      write_output(urls.join(\"\\n\") + \"\\n\")\n    else\n      urls.each do |i|\n        print_line(i)\n      end\n    end\n  end\nend\n",
    "x_mitre_contributors": [
        "[ mubix ]",
        ""
    ]
}