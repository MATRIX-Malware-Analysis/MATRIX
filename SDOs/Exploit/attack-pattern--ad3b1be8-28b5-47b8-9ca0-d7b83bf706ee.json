{
    "type": "attack-pattern",
    "spec_version": "2.1",
    "id": "attack-pattern--ad3b1be8-28b5-47b8-9ca0-d7b83bf706ee",
    "created": "2024-08-14T16:29:29.780647Z",
    "modified": "2024-08-14T16:29:29.780651Z",
    "name": "HTTP Robots.txt Content Scanner",
    "description": "Detect robots.txt files and analize its content",
    "external_references": [
        {
            "source_name": "metasploit",
            "url": "https://github.com/rapid7/metasploit-framework/blob/master/auxiliary/scanner/http/robots_txt.rb",
            "external_id": "robots_txt.rb"
        }
    ],
    "x_code_snippet": "##\n# This module requires Metasploit: https://metasploit.com/download\n# Current source: https://github.com/rapid7/metasploit-framework\n##\n\nclass MetasploitModule < Msf::Auxiliary\n\n  # Exploit mixins should be called first\n  include Msf::Exploit::Remote::HttpClient\n  include Msf::Auxiliary::WmapScanServer\n  # Scanner mixin should be near last\n  include Msf::Auxiliary::Scanner\n  include Msf::Auxiliary::Report\n\n  def initialize\n    super(\n      'Name'        => 'HTTP Robots.txt Content Scanner',\n      'Description' => 'Detect robots.txt files and analize its content',\n      'Author'       => ['et'],\n      'License'     => MSF_LICENSE\n    )\n\n    register_options(\n      [\n        OptString.new('PATH', [ true,  \"The test path to find robots.txt file\", '/']),\n\n      ])\n\n  end\n\n  def run_host(target_host)\n\n    tpath = normalize_uri(datastore['PATH'])\n    if tpath[-1,1] != '/'\n      tpath += '/'\n    end\n\n    begin\n      turl = tpath+'robots.txt'\n\n      res = send_request_raw({\n        'uri'     => turl,\n        'method'  => 'GET',\n        'version' => '1.0',\n      }, 10)\n\n\n      if not res\n        print_error(\"[#{target_host}] #{tpath}robots.txt - No response\")\n        return\n      end\n\n      if not res.body.include?(\"llow:\")\n        vprint_status(\"[#{target_host}] #{tpath}robots.txt - Doesn't contain \\\"llow:\\\"\")\n        return\n      end\n\n      print_status(\"[#{target_host}] #{tpath}robots.txt found\")\n      print_good(\"Contents of Robots.txt:\\n#{res.body}\")\n\n      # short url regex\n      aregex = /llow:[ ]{0,2}(.*?)$/i\n\n      result = res.body.scan(aregex).flatten.map{ |s| s.strip }.uniq\n\n      vprint_status(\"[#{target_host}] #{tpath}robots.txt - #{result.join(', ')}\")\n      result.each do |u|\n        report_note(\n          :host\t=> target_host,\n          :port\t=> rport,\n          :proto => 'tcp',\n          :sname\t=> (ssl ? 'https' : 'http'),\n          :type\t=> 'ROBOTS_TXT',\n          :data\t=> u,\n          :update => :unique_data\n        )\n      end\n\n    rescue ::Rex::ConnectionRefused, ::Rex::HostUnreachable, ::Rex::ConnectionTimeout\n    rescue ::Timeout::Error, ::Errno::EPIPE\n    end\n  end\nend\n"
}