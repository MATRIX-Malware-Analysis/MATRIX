{
    "type": "attack-pattern",
    "spec_version": "2.1",
    "id": "attack-pattern--474d49ab-8fc2-4175-b51e-129fdc9c2660",
    "created": "2024-08-14T16:26:30.962937Z",
    "modified": "2024-08-14T16:26:30.962942Z",
    "name": "Metasploit Web Crawler",
    "description": "This auxiliary module is a modular web crawler, to be used in conjunction with wmap (someday) or standalone.",
    "external_references": [
        {
            "source_name": "metasploit",
            "url": "https://github.com/rapid7/metasploit-framework/blob/master/auxiliary/crawler/msfcrawler.rb",
            "external_id": "msfcrawler.rb"
        }
    ],
    "x_code_snippet": "##\n# This module requires Metasploit: https://metasploit.com/download\n# Current source: https://github.com/rapid7/metasploit-framework\n##\n\n#\n# Web Crawler.\n#\n# Author:  Efrain Torres   et [at] metasploit.com 2010\n#\n#\n\n# openssl before rubygems mac os\nrequire 'openssl'\nrequire 'rinda/tuplespace'\nrequire 'pathname'\nrequire 'uri'\n\nclass MetasploitModule < Msf::Auxiliary\n  include Msf::Auxiliary::Scanner\n  include Msf::Auxiliary::Report\n\n  def initialize(info = {})\n    super(update_info(info,\n      'Name'\t\t\t=> 'Metasploit Web Crawler',\n      'Description'       => 'This auxiliary module is a modular web crawler, to be used in conjunction with wmap (someday) or standalone.',\n      'Author'\t\t\t=> 'et',\n      'License'\t\t\t=> MSF_LICENSE\n    ))\n\n    register_options([\n      OptString.new('PATH',\t[true,\t\"Starting crawling path\", '/']),\n      OptInt.new('RPORT', [true, \"Remote port\", 80 ])\n    ])\n\n    register_advanced_options([\n      OptPath.new('CrawlerModulesDir', [true,\t'The base directory containing the crawler modules',\n        File.join(Msf::Config.data_directory, \"msfcrawler\")\n      ]),\n      OptBool.new('EnableUl', [ false, \"Enable maximum number of request per URI\", true ]),\n      OptBool.new('StoreDB', [ false, \"Store requests in database\", false ]),\n      OptInt.new('MaxUriLimit', [ true, \"Number max. request per URI\", 10]),\n      OptInt.new('SleepTime', [ true, \"Sleep time (secs) between requests\", 0]),\n      OptInt.new('TakeTimeout', [ true, \"Timeout for loop ending\", 15]),\n      OptInt.new('ReadTimeout', [ true, \"Read timeout (-1 forever)\", 3]),\n      OptInt.new('ThreadNum', [ true, \"Threads number\", 20]),\n      OptString.new('DontCrawl',\t[true,\t\"Filestypes not to crawl\", '.exe,.zip,.tar,.bz2,.run,.asc,.gz'])\n    ])\n  end\n\n  attr_accessor :ctarget, :cport, :cssl\n\n  def run\n    i, a = 0, []\n\n    self.ctarget = datastore['RHOSTS']\n    self.cport = datastore['RPORT']\n    self.cssl = datastore['SSL']\n    inipath = datastore['PATH']\n\n    cinipath = (inipath.nil? or inipath.empty?) ? '/' : inipath\n\n    inireq = {\n        'rhost'\t\t=> ctarget,\n        'rport'\t\t=> cport,\n        'uri' \t\t=> cinipath,\n        'method'   \t=> 'GET',\n        'ctype'\t\t=> 'text/plain',\n        'ssl'\t\t=> cssl,\n        'query'\t\t=> nil,\n        'data'\t\t=> nil\n    }\n\n    @NotViewedQueue = Rinda::TupleSpace.new\n    @ViewedQueue = Hash.new\n    @UriLimits = Hash.new\n    @curent_site = self.ctarget\n\n    insertnewpath(inireq)\n\n    print_status(\"Loading modules: #{datastore['CrawlerModulesDir']}\")\n    load_modules(datastore['CrawlerModulesDir'])\n    print_status(\"OK\")\n\n    if datastore['EnableUl']\n      print_status(\"URI LIMITS ENABLED: #{datastore['MaxUriLimit']} (Maximum number of requests per uri)\")\n    end\n\n    print_status(\"Target: #{self.ctarget} Port: #{self.cport} Path: #{cinipath} SSL: #{self.cssl}\")\n\n\n    begin\n      reqfilter = reqtemplate(self.ctarget,self.cport,self.cssl)\n\n      i =0\n\n      loop do\n\n        ####\n        #if i <= datastore['ThreadNum']\n        #\ta.push(Thread.new {\n        ####\n\n        hashreq = @NotViewedQueue.take(reqfilter, datastore['TakeTimeout'])\n\n        ul = false\n        if @UriLimits.include?(hashreq['uri']) and datastore['EnableUl']\n          #puts \"Request #{@UriLimits[hashreq['uri']]}/#{$maxurilimit} #{hashreq['uri']}\"\n          if @UriLimits[hashreq['uri']] >= datastore['MaxUriLimit']\n            #puts \"URI LIMIT Reached: #{$maxurilimit} for uri #{hashreq['uri']}\"\n            ul = true\n          end\n        else\n          @UriLimits[hashreq['uri']] = 0\n        end\n\n        if !@ViewedQueue.include?(hashsig(hashreq)) and !ul\n\n          @ViewedQueue[hashsig(hashreq)] = Time.now\n          @UriLimits[hashreq['uri']] += 1\n\n          if !File.extname(hashreq['uri']).empty? and datastore['DontCrawl'].include? File.extname(hashreq['uri'])\n            vprint_status \"URI not crawled #{hashreq['uri']}\"\n          else\n              prx = nil\n              #if self.useproxy\n              #\tprx = \"HTTP:\"+self.proxyhost.to_s+\":\"+self.proxyport.to_s\n              #end\n\n              c = Rex::Proto::Http::Client.new(\n                self.ctarget,\n                self.cport.to_i,\n                {},\n                self.cssl,\n                nil,\n                prx\n              )\n\n              sendreq(c,hashreq)\n          end\n        else\n          vprint_line \"#{hashreq['uri']} already visited. \"\n        end\n\n        ####\n        #})\n\n        #i += 1\n        #else\n        #\tsleep(0.01) and a.delete_if {|x| not x.alive?} while not a.empty?\n        #\ti = 0\n        #end\n        ####\n\n      end\n    rescue Rinda::RequestExpiredError\n      print_status(\"END.\")\n      return\n    end\n\n    print_status(\"Finished crawling\")\n  end\n\n  def reqtemplate(target,port,ssl)\n    hreq = {\n      'rhost'\t\t=> target,\n      'rport'\t\t=> port,\n      'uri'  \t\t=> nil,\n      'method'   \t=> nil,\n      'ctype'\t\t=> nil,\n      'ssl'\t\t=> ssl,\n      'query'\t\t=> nil,\n      'data'\t\t=> nil\n    }\n\n    return hreq\n  end\n\n  def storedb(hashreq,response,dbpath)\n\n    # Added host/port/ssl for report_web_page support\n    info = {\n      :web_site => @current_site,\n      :path     => hashreq['uri'],\n      :query    => hashreq['query'],\n      :host     => hashreq['rhost'],\n      :port     => hashreq['rport'],\n      :ssl      => !hashreq['ssl'].nil?,\n      :data\t    => hashreq['data'],\n      :code     => response.code,\n      :body     => response.body,\n      :headers  => response.headers\n    }\n\n    #if response['content-type']\n    #\tinfo[:ctype] = response['content-type'][0]\n    #end\n\n    #if response['set-cookie']\n    #\tinfo[:cookie] = page.headers['set-cookie'].join(\"\\n\")\n    #end\n\n    #if page.headers['authorization']\n    #\tinfo[:auth] = page.headers['authorization'].join(\"\\n\")\n    #end\n\n    #if page.headers['location']\n    #\tinfo[:location] = page.headers['location'][0]\n    #end\n\n    #if page.headers['last-modified']\n    #\tinfo[:mtime] = page.headers['last-modified'][0]\n    #end\n\n    # Report the web page to the database\n    report_web_page(info)\n  end\n\n  #\n  # Modified version of load_protocols from psnuffle by Max Moser  <mmo@remote-exploit.org>\n  #\n\n  def load_modules(crawlermodulesdir)\n\n    base = crawlermodulesdir\n    if (not File.directory?(base))\n      raise RuntimeError,\"The Crawler modules parameter is set to an invalid directory\"\n    end\n\n    @crawlermodules = {}\n    cmodules = Dir.new(base).entries.grep(/\\.rb$/).sort\n    cmodules.each do |n|\n      f = File.join(base, n)\n      m = ::Module.new\n      begin\n        m.module_eval(File.read(f, File.size(f)))\n        m.constants.grep(/^Crawler(.*)/) do\n          cmod = $1\n          klass = m.const_get(\"Crawler#{cmod}\")\n          @crawlermodules[cmod.downcase] = klass.new(self)\n\n          print_status(\"Loaded crawler module #{cmod} from #{f}...\")\n        end\n      rescue ::Exception => e\n        print_error(\"Crawler module #{n} failed to load: #{e.class} #{e} #{e.backtrace}\")\n      end\n    end\n  end\n\n  def sendreq(nclient,reqopts={})\n\n    begin\n      r = nclient.request_raw(reqopts)\n      resp = nclient.send_recv(r, datastore['ReadTimeout'])\n\n      if resp\n        #\n        # Quickfix for bug packet.rb to_s line: 190\n        # In case modules or crawler calls to_s on de-chunked responses\n        #\n        resp.transfer_chunked = false\n\n        if datastore['StoreDB']\n          storedb(reqopts,resp,$dbpathmsf)\n        end\n\n        print_status \">> [#{resp.code}] #{reqopts['uri']}\"\n\n        if reqopts['query'] and !reqopts['query'].empty?\n          print_status \">>> [Q] #{reqopts['query']}\"\n        end\n\n        if reqopts['data']\n          print_status \">>> [D] #{reqopts['data']}\"\n        end\n\n        case resp.code\n        when 200\n          @crawlermodules.each_key do |k|\n            @crawlermodules[k].parse(reqopts,resp)\n          end\n        when 301..303\n          print_line(\"[#{resp.code}] Redirection to: #{resp['Location']}\")\n          vprint_status urltohash('GET',resp['Location'],reqopts['uri'],nil)\n          insertnewpath(urltohash('GET',resp['Location'],reqopts['uri'],nil))\n        when 404\n          print_status \"[404] Invalid link #{reqopts['uri']}\"\n        else\n          print_status \"Unhandled #{resp.code}\"\n        end\n\n      else\n        print_status \"No response\"\n      end\n      sleep(datastore['SleepTime'])\n    rescue\n      print_status \"ERROR\"\n      vprint_status \"#{$!}: #{$!.backtrace}\"\n    end\n  end\n\n  #\n  # Add new path (uri) to test non-viewed queue\n  #\n\n  def insertnewpath(hashreq)\n\n    hashreq['uri'] = canonicalize(hashreq['uri'])\n\n    if hashreq['rhost'] == datastore['RHOSTS'] and hashreq['rport'] == datastore['RPORT']\n      if !@ViewedQueue.include?(hashsig(hashreq))\n        if @NotViewedQueue.read_all(hashreq).size > 0\n          vprint_status \"Already in queue to be viewed: #{hashreq['uri']}\"\n        else\n          vprint_status \"Inserted: #{hashreq['uri']}\"\n\n          @NotViewedQueue.write(hashreq)\n        end\n      else\n        vprint_status \"#{hashreq['uri']} already visited at #{@ViewedQueue[hashsig(hashreq)]}\"\n      end\n    end\n  end\n\n  #\n  # Build a new hash for a local path\n  #\n\n  def urltohash(m,url,basepath,dat)\n\n      # m:   method\n      # url: uri?[query]\n      # basepath: base path/uri to determine absolute path when relative\n      # data: body data, nil if GET and query = uri.query\n\n      uri = URI.parse(url)\n      uritargetssl = (uri.scheme == \"https\") ? true : false\n\n      uritargethost = uri.host\n      if (uri.host.nil? or uri.host.empty?)\n        uritargethost = self.ctarget\n        uritargetssl = self.cssl\n      end\n\n      uritargetport = uri.port\n      if (uri.port.nil?)\n        uritargetport = self.cport\n      end\n\n      uritargetpath = uri.path\n      if (uri.path.nil? or uri.path.empty?)\n        uritargetpath = \"/\"\n      end\n\n      newp = Pathname.new(uritargetpath)\n      oldp = Pathname.new(basepath)\n      if !newp.absolute?\n        if oldp.to_s[-1,1] == '/'\n          newp = oldp+newp\n        else\n          if !newp.to_s.empty?\n            newp = File.join(oldp.dirname,newp)\n          end\n        end\n      end\n\n      hashreq = {\n        'rhost'\t\t=> uritargethost,\n        'rport'\t\t=> uritargetport,\n        'uri' \t\t=> newp.to_s,\n        'method'   \t=> m,\n        'ctype'\t\t=> 'text/plain',\n        'ssl'\t\t=> uritargetssl,\n        'query'\t\t=> uri.query,\n        'data'\t\t=> nil\n      }\n\n      if m == 'GET' and !dat.nil?\n        hashreq['query'] = dat\n      else\n        hashreq['data'] = dat\n      end\n\n      return hashreq\n  end\n\n  # Taken from http://www.ruby-forum.com/topic/140101 by  Rob Biedenharn\n  def canonicalize(uri)\n\n    u = uri.kind_of?(URI) ? uri : URI.parse(uri.to_s)\n    u.normalize!\n    newpath = u.path\n    while newpath.gsub!(%r{([^/]+)/\\.\\./?}) { |match|\n      $1 == '..' ? match : ''\n    } do end\n    newpath = newpath.gsub(%r{/\\./}, '/').sub(%r{/\\.\\z}, '/')\n    u.path = newpath\n    # Ugly fix\n    u.path = u.path.gsub(\"\\/..\\/\",\"\\/\")\n    u.to_s\n  end\n\n  def hashsig(hashreq)\n    hashreq.to_s\n  end\nend\n\nclass BaseParser\n  attr_accessor :crawler\n\n  def initialize(c)\n    self.crawler = c\n  end\n\n  def parse(request,result)\n    nil\n  end\n\n  #\n  # Add new path (uri) to test hash queue\n  #\n  def insertnewpath(hashreq)\n    self.crawler.insertnewpath(hashreq)\n  end\n\n  def hashsig(hashreq)\n    self.crawler.hashsig(hashreq)\n  end\n\n  def urltohash(m,url,basepath,dat)\n    self.crawler.urltohash(m,url,basepath,dat)\n  end\n\n  def targetssl\n    self.crawler.cssl\n  end\n\n  def targetport\n    self.crawler.cport\n  end\n\n  def targethost\n    self.crawler.ctarget\n  end\n\n  def targetinipath\n    self.crawler.cinipath\n  end\nend\n"
}