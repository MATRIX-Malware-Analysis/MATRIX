{
    "type": "vulnerability",
    "spec_version": "2.1",
    "id": "vulnerability--958a48d2-9863-4759-b2d4-0283fb9c6f0d",
    "created": "2024-08-13T16:13:04.830627Z",
    "modified": "2024-08-13T16:13:04.830631Z",
    "name": "No Title Available",
    "description": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
    "external_references": [
        {
            "source_name": "cve",
            "external_id": "CVE-2023-29374"
        }
    ],
    "references": [
        {
            "url": "https://github.com/hwchase17/langchain/pull/1119"
        },
        {
            "url": "https://github.com/hwchase17/langchain/issues/814"
        },
        {
            "url": "https://twitter.com/rharang/status/1641899743608463365/photo/1"
        },
        {
            "url": "https://github.com/hwchase17/langchain/issues/1026"
        }
    ]
}